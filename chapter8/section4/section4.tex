\documentclass[../../master.tex]{subfiles}

\DeclareMathOperator{\Rees}{Rees}

\begin{document}
\section{Multilinear algebra} 

% Problem 4.1
\begin{problem}
    Verify that the module $\Lambda_R^{\ell}(M)$ constructed in \S 4.2 does satisfy the universal property for alternating multilinear maps.
    Construct a module $\mathbb{S}_R^{\ell}(M)$ satisfying the universal property for symmetric multilinear maps.
\end{problem}

\begin{solution}
    Let $\varphi : M^{\ell} \to P$ be an alternating map.
    In particular, $\varphi$ is multilinear hence it factors through $\mathbb{T}_R^{\ell}(M)$, call this map $\bar{\varphi}$.
    Recall that we define the submodule $W \subseteq \mathbb{T}_R^{\ell}(M)$ to be generated by pure tensors $m_1 \otimes \cdots \otimes m_\ell$ where $m_i = m_j$ for some $i \neq j$
    Then it is clear that every element of $W$ is in the kernel of $\bar{\varphi}$, as for $m_1 \otimes \cdots \otimes m_\ell \in W$ we find
    \[
    \varphi(m_1 \otimes \cdots m_{\ell}) = 0
    \]
    by the assumption that $\varphi$ is alternating and that $m_i = m_j$ for some $i \neq j$.
    Therefore, $\bar{\varphi}$ factors uniquely through the quotient $\mathbb{T}_R^{\ell}(M) / W = \Lambda_R^{\ell}(M)$ and we are done.

    Similarly, let $W \subseteq \mathbb{T}_R^{\ell}(M)$ be the submodule generated by elements of the form $m_{\sigma(1)} \otimes \cdots \otimes m_{\sigma(\ell)} - m_1 \otimes \cdots \otimes m_\ell$ for all $\sigma \in S_\ell$.
    Then we claim $\mathbb{S}_R^{\ell} := \mathbb{T}_R^{\ell} / W$ satisfies the relevant universal property.
    Indeed, as above letting $\varphi : M^{\ell} \to P$ be a symmetric map, and in particular a multilinear map, shows that it factors through $\mathbb{T}_R^{\ell}(M)$ via $\bar{\varphi}$, which is a linear alternating map.
    Then every element of $W$ is in the kernel of $\bar{\varphi}$ by simply applying linearity.
    Indeed, we have
    \[
    \bar{\varphi}(m_{\sigma(1)} \otimes \cdots \otimes m_{\sigma(\ell)}) - \bar{\varphi}(m_1 \otimes \cdots \otimes m_\ell) = 0.
    \]
    Thus, by the universal property of quotients, the map factors through $\mathbb{T}_R^{\ell}(M) / W = \mathbb{S}_R^{\ell}(M)$.
\end{solution}

% Problem 4.2
\begin{problem}
    Define the actions of $\mathbb{T}_R^{\ell}, \mathbb{S}_R^{\ell}, \Lambda_R^{\ell}$ on $R$-linear maps, making covariant functors out of these notions.
\end{problem}

\begin{solution}
    Given an $R$-linear map $\varphi : M \to N$, define $\varphi^{\otimes \ell} : \mathbb{T}_R^{\ell}(M) \to \mathbb{T}_R^{\ell}(N)$ to be
    \[
    m_1 \otimes \cdots \otimes m_\ell \mapsto \varphi(m_1) \otimes \cdots \otimes \varphi(m_\ell)
    \]
    It can easily be seen in the case of $\ell = 2$ that the map is linear in each factor, hence multilinear in $M^{\ell}$.
    Indeed, we have
    \begin{align*}
        \varphi^{\otimes 2} (m_1 \otimes (m_2 + m_2')) &= \varphi(m_1) \otimes \varphi(m_2 + m_2') \\
                                                     &= \varphi(m_1) \otimes \varphi(m_2) + \varphi(m_1) \otimes \varphi(m_2') \\
                                                     &= \varphi(m_1 \otimes m_2) + \varphi(m_1 \otimes m_2')
    \end{align*}
    Thus, $\mathbb{T}_R^{\ell}$ can be seen as a covariant functor from $R\mathsf{-Mod}$ to itself.

    Admittedly, I'm not sure of a natural way to define the symmetric and exterior powers of an $R$-linear map.
    Instead, I'll just define it to be the $\varphi^{\otimes \ell}$ and map elements of the submodules used in defining $\mathbb{S}_R^{\ell}$ and $\Lambda_R^{\ell}$ to 0 respectively.
    Then these maps uniquely factor through the symmetric (resp. exterior) power, hence we take that unique map to be the action of the functor.
\end{solution}

% Problem 4.3
\begin{problem}
    Let $I$ be the ideal $(x, y)$ in $k[x, y]$;
    so every element of $I$ may be written (of course, not uniquely) in the form $fx + gy$ for some polynomials $f, g \in k[x, y]$.
    Define a function $\varphi: I \times I \to k$ by prescribing
    \[
    \varphi(f_1 x + g_1 y, f_2x + g_2 y) := f_1(0, 0) g_2(0, 0) - f_2(0, 0) g_1(0, 0).
    \]
    \begin{itemize}
        \item Prove that $\varphi$ is well-defined.
        \item Prove that $\varphi$ is $k[x, y]$-bilinear and alternating.
        \item Prove that $\Lambda_{k[x, y]}^2(I) \neq 0$.
    \end{itemize}
    Note that $I$ has rank 1 as a $k[x, y]$-module;
    if it were free, its second exterior power would have to vanish by Lemma 4.3.
\end{problem}

\begin{solution}
    To do.
\end{solution}

% Problem 4.4
\begin{problem}
    Let $F_1$ and $F_2$ be free $R$-modules of finite rank.
    \begin{itemize}
        \item Construct a `meaningful' isomorphism $\det(F_1) \otimes \det(F_2) \cong \det(F_1 \oplus F_2)$.
        \item More generally, prove that
            \[
            \Lambda_R^{r}(F_1 \oplus F_2) \cong \bigoplus_{i+j=r} (\Lambda_R^{i} F_1) \otimes_R (\Lambda_R^{j} F_2).
            \]
    \end{itemize}
\end{problem}

\begin{solution}
    Recall that if $F$ has rank $r$, then $\det(F) := \Lambda_R^{r}(F)$.
    Suppose $F_1$ has rank $s_1$ and $F_2$ has rank $s_2$.
    Consider the map $\det(F_1) \otimes \det(F_2) \to \det(F_1 \oplus F_2)$ which sends
    \[
        (a_1 \wedge \cdots \wedge a_{s_1}) \otimes (b_1 \wedge \cdots \wedge b_{s_2}) \mapsto a_1 \wedge a_{s_1} \wedge b_1 \wedge \cdots \wedge b_{s_2}.
    \]
    That is, it merely concatenates pure wedges and can be extended by linearity.
    It is clear that the latter is in $\det(F_1 \oplus F_2) = \Lambda_R^{s_1+s_2}(F_1 \oplus F_2)$
    I'm not really sure if this isomorphism is `meaningful' but hey, it's an isomorphism :)

    For the more general case, let $1 \leq k_1 < \cdots < k_r \leq s_1 + s_2$.
    Consider the map $\varphi_i$ which sends
    \[
    e_{k_1} \wedge \cdots \wedge e_{k_r} \mapsto e_{k_1} \wedge \cdots \wedge e_{k_i} \otimes e_{k_{i+1}} \wedge \cdots \wedge e_{k_r}
    \]
    Clearly the latter is in $\Lambda_R^{i}F_1 \otimes \Lambda_R^{j}F_2$, so defining $\varphi_i$ for all $0 \leq i \leq r$ yields the desired isomorphism.
\end{solution}

% Exercise 4.5
\begin{problem}
    Verify that the multilinear map $\varphi_I$ defined in the proof of Lemma 4.3 is alternating.
\end{problem}

\begin{solution}
    We recall how the map is defined.
    Let $I = (i_1, \ldots, i_\ell)$ with $1 \leq i_1 < \cdots < i_\ell \leq r$ and consider the map
    \[
    \varphi_I(e_{j_1}, \ldots, e_{j_\ell}) =
    \begin{cases}
        (-1)^{\sigma} \quad \text{ if $\exists$ a permutation $\sigma$ such that $\sigma(j_k) = i_k$, for all $k$,} \\
            0 \qquad \quad \text{ otherwise}
    \end{cases}
    \]
    and extending by linearity.

    Then it is clear that $\varphi_I$ is alternating because all of the $i_1, \ldots, i_\ell$ are necessarily distinct.
    In particular, if $j_{k_1} = j_{k_2}$ for some $k_1 \neq k_2$ then there is no permutation such that $\sigma(j_k) = i_k$ for all $k$ since it will send $j_{k_1}$ and $j_{k_2}$ to the same element.
    Thus, it is equal to 0 in this case, hence it is alternating.
\end{solution}

% Exercise 4.6
\begin{problem}
    Let $V$ be a vector space, and let $v_1, \ldots, v_\ell \in V$.
    Prove that $v_1, \ldots, v_\ell$ are linearly independent if and only if $v_1 \wedge \cdots \wedge v_\ell \neq 0$.
\end{problem}

\begin{solution}
    First suppose the vectors are linearly dependent.
    That is, $v_\ell$ is a linear combination of the other vectors.
    Substituting this linear combination for $v_\ell$ in the exterior product yields $v_1 \wedge \cdots \wedge v_\ell = 0$.
    Thus, if $v_1 \wedge \cdots \wedge v_\ell \neq 0$, the vectors are linearly independent.

    Now suppose $v_1 \wedge \cdots \wedge v_\ell = 0$.
    That is, every alternating map from $V$ vanishes at $v_1 \wedge \cdots \wedge v_\ell$.
    In particular, the determinant map from $V$ to $k$ vanishes, but this is true if and only if the vectors are linearly dependent.
\end{solution}

% Exercise 4.7
\begin{problem}
    Let $V$ be a $k$-vector space, and let $\{v_1, \ldots, v_\ell\}$, $\{w_1, \ldots, w_\ell\}$ be two sets of linearly independent vectors in $V$.
    Prove that $\{v_1, \ldots, v_\ell\}$, $\{w_1, \ldots, w_\ell\}$ span the same subspace in $V$ if and only if $v_1 \wedge \cdots \wedge v_\ell$ and $w_1 \wedge \cdots \wedge w_\ell$ are nonzero multiples of each other in $\Lambda_k^{\ell}V$.
    (For the interesting direction, if $\langle v_1, \ldots, v_\ell \rangle \neq \langle w_1, \ldots, w_\ell \rangle$, there must be a vector $u$ belonging to the first subspace but not to the second.
    What can you say about $(v_1 \wedge \cdots \wedge v_\ell) \wedge u$ and $(w_1 \wedge \cdots \wedge w_\ell) \wedge u$ in $\Lambda_k^{*}V$?)

    Deduce that there is an injective function from the Grassmannian of $\ell$-dimensional subspaces of $V$ (Exercise VI.2.13) to the projective space $\mathbb{P}(\Lambda_k^{\ell}V)$:
    the Grassmannian is identified with the set of `pure wedges' in the projectivization of the exterior power $\Lambda_k^{\ell}V$.
    This is called the \textit{Pl\"ucker embedding} of the Grassmannian.
\end{problem}

\begin{solution}
    First suppose the two sets span the same subspace.
    In particular every $w_i$ is a linear combination of the $v_k$.
    Writing the exterior product of the $w_i$ and using multilinearity properties shows that $w_1 \wedge \cdots \wedge w_\ell = c \cdot v_1 \wedge \cdots \wedge v_\ell$.
    Furthermore, $c \neq 0$ since the vectors are linearly independent (see Exercise 4.6).

    For the other direction, suppose the two sets of vectors don't span the same space.
    Let $u$ be a vector belonging to the span of $\{v_i\}$ which does not belong to the span of $\{w_i\}$.
    Then, the exterior product $(v_1 \wedge \cdots \wedge v_\ell) \wedge u = 0$ since the vectors are linearly independent, but the exterior product $(w_1 \wedge \cdots \wedge w_\ell) \wedge u \neq 0$ because the vectors are linearly independent.
    But if $v_1 \wedge \cdots \wedge v_\ell = c \cdot w_1 \wedge \cdots \wedge w_\ell$, we would have
    \[
        0 = v_1 \wedge \cdots \wedge v_\ell \wedge u = c \cdot w_1 \wedge \cdots \wedge w_\ell \wedge u \neq 0,
    \]
    a contradiction.
    Thus, the exterior products are not scalar multiples of each other.

    As a corollary, this yields an injective map from $\Gr(\ell, V) \hookrightarrow \mathbb{P}(\Lambda_k^{\ell}V)$ by sending a basis $\{v_1, \ldots, v_\ell\}$ of a subspace $W$ to the exterior product $v_1 \wedge \cdots \wedge v_\ell$.
    Indeed, choosing a different basis would yield scalar multiples of this exterior product, hence they become equal after projectivizing.
\end{solution}

% Exercise 4.8
\begin{problem}
    The Pl\"ucker embedding described in Exercise 4.7 realizes the Grassmannian $\Gr_k(2, 4)$ of 2-dimensional subspaces of $k^{4}$ as a subset of the projectivization of $\Lambda_k^{2} k^{4} \cong k^{6}$: $\Gr_k(2, 4) \subseteq \mathbb{P}_k^{5}$.
    Choose projective coordinates (Exercise VII.2.20) $(x_{12} : x_{13} : x_{14} : x_{23} : x_{24} : x_{34})$ in $\mathbb{P}(\Lambda_k^{2} k^{4})$, listed according to the corresponding minors, as in Example 4.5.
    Prove that $\Gr_k(2, 4)$ is the locus of zeros
    \[
    \mathscr{V}(x_{12} x_{34} - x_{13} x_{24} + x_{14} x_{23})
    \]
    (notation as in Exercise VII.2.21).
    (Remember that you know how to write every point of $\Gr_k(2, 4)$: look back at Exercise VI.2.14.)

    Thus, $\Gr_k(2, 4)$ may be viewed as a projective algebraic set.
    In fact, all Grassmannians may be similarly realized as projective algebraic sets (in the sense of Exercise 4.11) via the corresponding Pl\"ucker embeddings.

    Prove that $\Gr_k(2, 4)$ may be covered with six copies of $\mathbb{A}_k^{4}$.
    (Recall from Exercise VII.2.20 that $\mathbb{P}_k^{5}$ may be covered with six copies of $\mathbb{A}_k^{5}$;
    prove that the intersection of each with $\Gr_k(2, 4)$ may be identified with $\mathbb{A}_k^{4}$ in a natural way.)
\end{problem}

\begin{solution}
    It is clear that every element of $\Gr_k(2, 4)$ is in $\mathscr{V}(x_{12} x_{34} - x_{13} x_{24} + x_{14} x_{23})$.
    To see this, it suffices to take an arbitrary element of $\Gr_k(2, 4)$, say
    \[
    \begin{pmatrix}
        1 & 0 \\
        0 & 1 \\
        a & b \\
        c & d
    \end{pmatrix}
    \]
    and moving it to $\mathbb{P}_k^{5}$ via our choice of projective coordinates, yielding
    \[
    F(1 : b : d : -a : -c : ad - bc) = (ad - bc) - (-bc) + (-da) = 0.
    \]
    Similar verifications hold for the other points in $\Gr_k(2, 4)$.
    Since we have six linear independent elements after embedding $\Gr_k(2, 4) \hookrightarrow \mathbb{P}_k^{5}$, these span the solution set of the polynomial, hence every zero of the polynomial is in $\Gr_k(2, 4)$.

    To cover $\Gr_k(2, 4)$, start with our cover of $\mathbb{P}_k^{5}$ with six copies of $\mathbb{A}_k^{5}$ via the embeddings $(c_1, \ldots, c_5) \mapsto (1 : c_1 : \ldots : c_5)$ as the 1 ranges over the coordinates.
    In paritcular, we may consider the intersection of these with our embedding $\Gr_k(2, 4) \hookrightarrow \mathbb{P}_k^{5}$.
    For example, taking the above embedding, we may identify this with the image of the element of the Grassmannian stated above, namely $(1 : b : d : -a : -c : ad - bc)$.
    Since this has $4$ free elements, there is a natural identification with $\mathbb{A}_k^{4}$.
    Similar choices can be made for the remaining elements, identifying the $i$-th copy of $\mathbb{A}_k^{5}$ with the elements in the Grassmannian whose image in $\mathbb{P}_k^{5}$ have a 1 in the $i$-th coordinate.

    With that said, I have a feeling this isn't the right way to think about it.
    After all, the image of another element in the Grassmannian is $(0 : 1 : c : a : ac : -b)$ which only has 3 free elements, hence it is likely more reasonable to identify it with $\mathbb{A}_k^{3}$, but I'm not entirely sure.
\end{solution}

% Exercise 4.9
\begin{problem}
    Assume $2$ is a unit in $R$, and let $F$ be a free $R$-module of finite rank.
    \begin{itemize}
        \item Define a function $\lambda : \Lambda_R^{2}(F) \to \mathbb{T}_R^{2}(F)$ on a basis $e_i \wedge e_j$, $i < j$, by setting $\lambda(e_i \wedge e_j) = \frac{1}{2} (e_i \otimes e_j - e_j \otimes e_i)$ and extending by linearity.
            Prove that $\lambda$ is an injective homomorphism of $R$-modules and $\lambda(f_1 \wedge f_2) = \frac{1}{2}(f_1 \otimes f_2 - f_2 \otimes f_1)$ for all $f_1, f_2 \in F$.
        \item Define a function $\sigma : \mathbb{S}_r^2(F) \to \mathbb{T}_R^2(F)$ on a basis $e_i \otimes e_j$, $i \leq j$, by setting $\sigma(e_i \otimes e_j) = \frac{1}{2} (e_i \otimes e_j + e_j \otimes e_i)$ and extending by linearity.
            Prove that $\sigma$ is an injective homomorphism of $R$-modules and $\sigma(f_1 \otimes f_2) = \frac{1}{2}(f_1 \otimes f_2 + f_2 \otimes f_1)$ for all $f_1, f_2 \in F$.
        \item Prove that $\lambda$ identifies $\Lambda_R^2(F)$ with the kernel of the map $\mathbb{T}_R^2(F) \to \mathbb{S}_R^2(F)$ and $\sigma$ identifies $\mathbb{S}_R^2(F)$ with the kernel of the map $\mathbb{T}_R^2(F) \to \Lambda_R^2(F)$.
    \end{itemize}
    In particular, there is a `meaningful' isomorphism $F \otimes_R F \cong \Lambda_R^2(F) \oplus \mathbb{S}_R^2(F)$.
\end{problem}

\begin{solution}
    It is clear that $\lambda$ is a $R$-module homomorphism.
    To see that $\lambda$ is injective, suppose $\lambda(a \wedge b) = \frac{1}{2}(a \otimes b - b \otimes a) = 0$.
    Then $a \otimes b = b \otimes a \Longrightarrow a = b$, but then $a \wedge b = 0$ so $\lambda$ is injective.
    It is easy to see that $\lambda(f_1 \wedge f_2) = \frac{1}{2} (f_1 \otimes f_2 - f_2 \otimes f_1)$ by mere computation (or induction on the rank of $F$).

    Similarly, it is clear that $\sigma$ is a module homomorphism.
    Suppose $\sigma(a \otimes b) = \frac{1}{2} (a \otimes b + b \otimes a) = 0$.
    Then $a \otimes b = -b \otimes a$, but $a \otimes b = b \otimes a$, hence $a \otimes b = 0$.
    As above, it is easy to see that $\lambda(f_1 \otimes f_2) = \frac{1}{2}(f_1 \otimes f_2 + f_2 \otimes f_1)$ by computation.

    Let $W$ denote the kernel of the map $\mathbb{T}_R^2(F) \to \mathbb{S}_R^2(F)$.
    It is clear that $\im(\lambda) \subseteq W$ as $W$ is generated by $e_i \otimes e_j - e_j \otimes e_i$.
    For the other direction, note that the generators of $W$ are in the image of the map, hence $W \subseteq \im(\lambda)$.
    Thus, $\lambda$ identifies $\Lambda_R^2(F)$ with $W$.
    Similarly, it is clear that $\im(\sigma) \subseteq W'$ where $W'$ is the submodule generated by $e_1 \otimes e_1$ and $e_2 \otimes e_2$, and the other direction is just as above.
    This yields a clear identification $F \otimes_R F \cong \Lambda_R^2(F) \oplus \mathbb{S}_R^2(F)$.
    
\end{solution}

% Exercise 4.10
\begin{problem}
    Prove the equivalence (ii) $\Longleftrightarrow$ (iii) in Lemma 4.9.
    \begin{proposition}[Lemma 4.9] 
        Let $S = \bigoplus_i S_i$ be a graded ring and let $I \subseteq S$ be an ideal of $S$.
        Then the following are equivalent:
        \begin{enumerate}[label=(\roman*)] 
            \item $I$ is homogeneous;
            \item if $s \in S$ and $s = \sum_{i} s_i$ is the decomposition of $s$ into \text{homogeneous} elements $s_i \in S_i$, then $s \in I \Longleftrightarrow s_i \in I$ for all $i$;
            \item $I$ admits a generating set consisting of homogeneous elements;
            \item $I$ is the kernel of a graded homomorphism.
        \end{enumerate}
    \end{proposition}
\end{problem}

\begin{solution}
    If every element $s \in I$ admits a decomposition into homogeneous elements, then simply choose a generating set consisting of the homogeneous elements which appear in the decomposition of elements of $I$.
    For the other direction, if $I$ admits a generating set of homogeneous elements then in particular, every element of $I$ is a sum of homogeneous elements, which is exactly the statement of $(ii)$.
\end{solution}

% Exercise 4.11
\begin{problem}
    Let $I \subseteq k[x_0, \ldots, x_n]$ be a \textit{homogeneous} ideal.
    Prove that the condition ` $F(c_0, \ldots, c_n) = 0$ for all $F \in I$' for a point $(c_0 : \ldots : c_n) \in \mathbb{P}_k^{n}$ is well-defined:
    it does not depend on the representative $(c_0, \ldots, c_n)$ chosen for the point $(c_0 : \ldots : c_n)$.
    (Cf. Exercise VII.2.21, but note that not all $F$ in $I$ are homogeneous.)

    Thus, every homogeneous ideal $I \subseteq k[x_0, \ldots, x_n]$ determines a `projective algebraic set'
    \[
    \mathscr{V}(I) := \{(c_0 : \ldots : c_n) \in \mathbb{P}_K^{n} \mid (\forall F \in I), f(c_0, \ldots, c_n) = 0\}.
    \]
    Note that $\mathscr{V}((x_0, \ldots, x_n)) = \emptyset$.
    The ideal $(x_0, \ldots, x_n) = \bigoplus_{i > 0} k[x_0, \ldots, x_n]_i$ is irreverently called the \textit{irrelevant ideal}.
\end{problem}

\begin{solution}
    This is almost entirely analogous to Exercise VII.2.21.
    Let $F \in I$.
    By Lemma 4.9, $F$ can be decomposed into the sum of homogeneous polynomials.
    But then for each of these homogeneous polynomials, the choice of representative does not matter as we can merely factor out the scalar multiple associated with various representatives.
\end{solution}

% Exercise 4.12
\begin{problem}
    Prove the `weak homogeneous Nullstellensatz':
    if $k$ is algebraically closed and $I \subseteq k[x_0, \ldots, x_n]$ is a homogeneous ideal, then $\mathscr{V}(I) = \emptyset$ if and only if $\sqrt{I}$ is either $k[x_0, \ldots, x_n]$ or the irrelevant ideal $(x_0, \ldots, x_n)$.
    (Translate this into a question about $\mathbb{A}_k^{n+1}$.)
\end{problem}

\begin{solution}
    It is clear that $\mathscr{V}((x_0, \ldots, x_n)) = \emptyset$ as the coordinate $(0 : \ldots : 0) \notin \mathbb{P}_k^{n}$ and this is the only point where all functions in the ideal vanish.
    Similarly, if $\sqrt{I} = k[x_0, \ldots, x_n]$, then in particular $1 \in I$.
    But $1$ vanishes nowhere, hence $\mathscr{V}(I) = \emptyset$.

    For the other direction, suppose $\mathscr{V}(I) = \emptyset$ and $\sqrt{I} \neq (x_0, \ldots, x_n)$.
    Consider the set $\mathscr{V}'(I) \in \mathbb{A}_k^{n+1}$ to be the set of points $p = (p_0, \ldots, p_{n})$ such that $f(p) = 0$ for all $f \in I$.
    If $\mathscr{V}'(I)$ is nonempty, then the projectivization of that point is in the projective variety of $I$, contradicting the assumption that it is empty.
    Thus, $\mathscr{V}'(I)$ is empty, but by the affine Nullstellensatz, this implies that $I = (1)$, hence $\sqrt{I} = k[x_0, \ldots, x_n]$.
\end{solution}

% Exercise 4.13
\begin{problem}
    Let $k$ be a field of characteristic zero.
    A \emph{differential operator} in one variable, with polynomial coefficients, is a linear combination
    \[
    \tag{*} a_0(x) + a_1(x) \partial_x + \cdots + a_r(x) \partial_x^{r}
    \]
    where $a_i(x) \in k[x]$.
    Here $x$ acts on a polynomial $f(x) \in k[x]$ by multiplication by $x$, while $\partial_x$ acts by taking a formal derivative (as in \S VII.4.2).
    With the evident operations, differential operators form a ring, called the (first) \textit{Weyl algebra}.
    Note that the Weyl algebra is noncommutative: for example,
    \[
        (x\partial_x) f(x) = x f'(x),
    \]
    while
    \[
        (\partial_x x) f(x) = \partial_x (xf(x)) = f(x) + x f'(x).
    \]
    Therefore, $(\partial_x x - x \partial_x) f(x) = f(x)$, or put otherwise
    \[
    \tag{**} \partial_x x - x \partial_x = 1
    \]
    in the Weyl algebra.
    Prove that the Weyl algebra is isomorphic to the quotient $\mathbb{T}_k^{*}(\langle x, y \rangle) / (yx - xy - 1)$.
    (Use ({**}) to write any element of the Weyl algebra in the form given in (*).
    Show that this representation is unique, and deduce that (**) generates all relations among $x$ and $\partial_x$.
    Then use the first isomorphism theorem.)

    The relation (**) expresses (up to a factor) the basic fact that in quantum mechanics the position and momentum operators do not commute.
    The Weyl algebra was introduced to study this phenomenon.
    Modules over rings of differential operators are called \textit{D-modules}.
\end{problem}

\begin{solution}
    For any element of the Weyl algebra, we may rewrite a factor of $\partial_x x$ as $ 1 + x \partial_x$.
    Doing so repeatedly and applying linearity eventually yields an element of the form given in (*).
    Honestly, I don't know how to show that this representation is unique.
    Assuming that it is, this yields a surjective map from $\mathbb{T}_k^{*}(\langle x, y \rangle)$ to the Weyl algebra sending $x \otimes 1 \mapsto x$ and $1 \otimes y \mapsto \partial_x$.
    Then the kernel of this map is precisely the set of elements mapped to the relation $\partial_x x - x \partial_x - 1$, but this is simply $yx - xy - 1$.
    Thus, by the first isomorphism theorem we obtain the desired isomorphism.
\end{solution}

% Exercise 4.14
\begin{problem}
    Let $F$ be a free $R$-module of rank $r$.
    Prove that $\mathbb{S}_R^{\ell}(F)$ is free, and compute its rank.
\end{problem}

\begin{solution}
    A basis for $\mathbb{S}_R^{\ell}(F)$ is the symmetrizations of $v_{i_1} \otimes \cdots \otimes v_{i_\ell}$ for $1 \leq i_1 \leq \cdots \leq i_\ell \leq r$.
    Indeed, any pure tensor determines a symmetric tensor via summing over all permutations ofthe indices, and two pure tensors have the same symmetric tensor if their indices determine the same multiset.
    Furthermore, it is clear that they are linearly independent (applying a similar argument as seen in the chapter).
    The number of sequences that can be chosen in this manner is (via stars and bars method)
    \[
    {r - \ell + 1} \choose \ell
    \]
\end{solution}

% Exercise 4.15
\begin{problem}
    Let $F_1, F_2$ be free $R$-modules of finite rank.
    Prove that $\mathbb{S}_R^{*}(F_1 \oplus F_2) \cong \mathbb{S}_R^{*}(F_1) \otimes_R \mathbb{S}_R^{*}(F_2)$.
\end{problem}

\begin{solution}
    Just as in Exercise 4, we have an isomorphism
    \[
    \mathbb{S}_R^{r}(F_1 \oplus F_2) \cong \bigoplus_{i + j = r} (\mathbb{S}_R^{i} F_1) \otimes_R (\mathbb{S}_R^{j} F_2)
    \]
    by tensoring pure symmetric tensors.
    But then we have an isomorphism
    \begin{align*}
        \mathbb{S}_R^{*}(F_1 \oplus F_2) &\cong \bigoplus_{r \geq 0} \mathbb{S}_R^{r}(F_1 \oplus F_2) \\
                                         &\cong \bigoplus_{r \geq 0} \bigoplus_{i+j=r} (\mathbb{S}_R^{i} F_1) \otimes_R (\mathbb{S}_R^{j} F_2) \\
                                         &\cong \mathbb{S}_R^{*}(F_1) \otimes_R \mathbb{S}_R^{*}(F_2).
    \end{align*}
\end{solution}

% Exercise 4.16
\begin{problem}
    Verify the skew commutativity of the exterior algebra, stated in Remark 4.11.
\end{problem}

\begin{solution}
    The multiplication in the exterior algebra is given by concatenation of exterior products.
    Suppose $\alpha \in \Lambda_R^{i}(M)$ and $\beta \in \Lambda_R^{j}(M)$.
    Then $\alpha \wedge \beta$ can be rewritten to look like $\beta \wedge \alpha$ by moving each element of $\alpha$ one place to the right for each element of $\beta$.
    That is, we make $j$ swaps for each of the $i$ elements of $\alpha$, hence we have
    \[
    \alpha \wedge \beta = (-1)^{ij} \beta \wedge \alpha.
    \]
\end{solution}

% Exercise 4.17
\begin{problem}
    Let $V$ be a $k$-vector space of dimension $r$.
    Prove that, as a vector space, the exterior algebra $\Lambda_k^{*}(V)$ has dimension $2^{r}$.
\end{problem}

\begin{solution}
    Recall that the exterior power $\Lambda_k^{\ell}(V)$ has dimension ${r \choose \ell}$.
    Summing over all $0 \leq \ell \leq r$ yields
    \[
    \sum_{\ell = 0}^{r} {r \choose \ell} = 2^{r}.
    \]
\end{solution}

% Exercise 4.18
\begin{problem}
    Prove that the Rees algebra $\Rees_R(I)$ of an ideal $I$ is Noetherian if $R$ is Noetherian.
\end{problem}

\begin{solution}
    Recall that the Rees algebra is $\Rees_R(I) = \bigoplus_{\ell \geq 0} I^{\ell}$ where $I^{0} = R$.
    Suppose $R$ is Noetherian so $I$ is finitely generated, say $I = \langle a_1, \ldots, a_n \rangle$.
    Note that each $I^{\ell}$ is finitely generated for all $\ell \geq 0$ by taking $\ell$-fold products of elements in the generating set.
    Consider the map $R[x_1, \ldots, x_n] \to \Rees_R(I)$ which sends $x_i \mapsto a_i$.
    Clearly this map is surjective, hence $\Rees_R(I)$ is a quotient of the polynomial ring.
    That is, $\Rees_R(I)$ is Noetherian if $R[x_1, \ldots, x_n]$ is Noetherian.
    But we know the latter is Noetherian by Hilbert's basis theorem.
\end{solution}

% Exercise 4.19
\begin{problem}
    Let $R$ be a Noetherian ring, $I$ an ideal of $R$, and consider the Rees algebra $\Rees_R(I) = \bigoplus_{\ell \geq 0} I^{\ell}$.
    By Exercise 4.18, $\Rees_R(I)$ is Noetherian.
    \begin{itemize}
        \item For $\ell \geq 0$, let $J_{\ell} \subseteq I^{\ell}$ be ideals of $R$, and view $J := \bigoplus_{\ell \geq 0} J_{\ell}$ as a sub-$R$-module of $\Rees_R(I)$.
            Prove that $J$ is an ideal of $\Rees_R(I)$ if and only if $I^{n} J_\ell \subseteq J_{\ell + n}$ for all $\ell, n \geq 0$.
        \item Assume $J := \bigoplus_{\ell \geq 0} J_\ell$ is an ideal of $\Rees_R(I)$.
            Prove that $J$ admits a finite set of homogeneous generators.
        \item Choose a finite set of homogeneous generators for $J$, and let $s$ be the largest degree of an element in this set.
            Prove that $J_{s+1} \subseteq I^{s+1}J_0 + I^{s}J_1 + \cdots + IJ_s$ (as ideals of $R$).
        \item Prove that $J_{s+\ell} = I^{\ell}J_s$ for all $\ell \geq 0$.
    \end{itemize}
    This is a particular case of the `Artin-Rees' lemma.
\end{problem}

\begin{solution}
    First note that $J$ has a natural graded structure as well.
    Fixing $\ell \geq 0$, suppose $J$ is an ideal of $\Rees_R(I)$.
    That is, for all $x \in \Rees_R(I)$, $xJ \subseteq J$.
    In particular, we have $I^{n} J_\ell \subseteq I^{n} I^{\ell} \subseteq I^{\ell + n}$ should be contained in $J_{\ell + n}$.
    For the other direction, fix $n \geq 0$ and let $\ell$ vary.
    Suppose $I^{n} J_\ell \subseteq J_{\ell + n}$ for all $\ell \geq 0$.
    Let $x = x_0 + x_1 + \cdots + x_n \in \Rees_R(I)$ and consider
    \[
    xJ_\ell = x_0J_\ell + x_1J_\ell + \cdots + x_n J_\ell \subseteq J_\ell + J_{\ell + 1} + \cdots + J_{\ell + n} \subseteq J.
    \]
    Since this holds for all $\ell$, $J$ is an ideal of $\Rees_R(I)$.

    Now we show that $J$ admits a finite set of homogeneous generators.
    Note that $J$ is finitely generated By Exercise 4.10, it suffices to show that $J$ is homogeneous.
    Indeed, we have $J = \bigoplus_\ell J_\ell = \bigoplus_\ell (J \cap I^{\ell})$ since $J_\ell \subseteq I^{\ell}$ for all $\ell \geq 0$.

    Letting $s$ be the largest degree of an element in a finite set of homogeneous generators, we find that every element $x \in J_{s+1}$ has a finite decomposition into homogeneous components with degree $\leq s$.
    Furthermore, by the first point of this exercise, if $\ell + n = s+1$ then $I^{\ell} J_n \subseteq J_{s+1}$.
    That is, we can write 
    \[
    x \in \bigoplus_{\ell + n = s+1} I^{\ell} J_n \subseteq J_{s+1}
    \]
    In particular, since this holds for all $x \in J_{s+1}$ we have the equality $J_{s+1} = \bigoplus_{\ell + n = s+1} I^{\ell} J_n$.
    
    An entirely analogous argument holds in the case $J_{s+\ell}$ for all $\ell \geq 0$.
\end{solution}

% Exercise 4.20
\begin{problem}
    Let $R$ be a Noetherian ring, and let $I$ be an ideal of $R$.
    Prove that $I \cdot \bigcap_{n \geq 0} I^{n} = \bigcap_{n \geq 0} I^{n}$.
    (Hint: Exercise 4.19.)
\end{problem}

\begin{solution}
    Clearly the $\subseteq$ direction holds as $I \cdot I^{n} \subseteq R \cdot I^{n} \subseteq I^{n}$ for all $n \geq 0$.
    By the above Exercise, we have $I^{n+1} = I \cdot I^{n}$ for all $n \geq 0$.
    Taking intersections, this implies $\bigcap_{n \geq 0} I^{n+1} = I \cdot \bigcap_{n \geq 0} I^{n}$.
    But clearly $I^{n+1} \subseteq R = I^{0}$ for all $n \geq 0$, hence we find
    \[
    I \cdot \bigcap_{n \geq 0} I^{n} \subseteq \bigcap_{n \geq 1} I^{n}
    \]
    proving equality.
\end{solution}

% Exercise 4.21
\begin{problem}
    Let $k$ be a field, and consider the ideal $I = (x, y)$ in the ring $R = k[x, y]$.
    Prove that the Rees algebra $\bigoplus_{j \geq_0} I^{j}$ is isomorphic to the quotient of a polynomial ring $k[x, y, s, t]$ by the ideal $(tx - sy)$.
    Deduce that in this case, the Rees algebra of $I$ is isomorphic to the symmetric algebra $\mathbb{S}_{k[x, y]}^{*}(I)$.

    (For the last point, use the exact sequence mentioned at the end of $\S 4.4$.
    It will be helpful to have an explicit presentation of $I$ as a $k[x, y]$-module;
    for this, looking back at Exercise VI.4.15 may help.)
\end{problem}

\begin{solution}
    Consider the map $\varphi:k[x, y, s, t] \mapsto \Rees_{k[x,y]}(x, y)$ which sends $x \mapsto (x, 0, \ldots)$, $y \mapsto (y, 0, \ldots)$, $s \mapsto (0, x, \ldots)$, and $t \mapsto (0, y, \ldots)$.
    Since the generators of the Rees algebra are mapped onto, this map is surjective.
    The kernel is the set of elements sent to 0.
    We claim that $\ker(\varphi) = (tx - sy)$.
    Indeed, we have
    \begin{align*}
        \varphi(tx - sy) &= \varphi(t) \varphi(x) - \varphi(s) \varphi(y) \\
                         &= (0, y, \ldots) \cdot (x, 0, \ldots) - (0, x, \ldots) \cdot (y, 0, \ldots) \\
                         &= (0, xy, \ldots) - (0, xy, \ldots) \\
                         &= 0.
    \end{align*}
    For the other direction, note that this is the only relation among the generators mapped to 0.
    Then by the unique factorization of $k[x, y, s, t]$ every element in the kernel contains a factor of $(tx - sy)$.
    (90\% sure this is wrong but oh well.)

    For the second part, let $R = k[x, y]$ and consider the exact sequence
    \[
    \begin{tikzcd}
        0 & R & R^2 & I & 0
        \arrow[from=1-1, to=1-2] 
        \arrow[from=1-2, to=1-3, "d_2"] 
        \arrow[from=1-3, to=1-4, "d_1"] 
        \arrow[from=1-4, to=1-5] 
    \end{tikzcd}
    \]
    where
    \[
    d_1 = 
    \begin{pmatrix}
        x & y
    \end{pmatrix}, \quad
    d_2 =
    \begin{pmatrix}
        y \\
        -x
    \end{pmatrix}
    \]
    Then applying the symmetric algebra functor yields the exact sequence
    \[
    \begin{tikzcd}
        0 & R \cdot \mathbb{S}_R^{*-1}(R^2) & \mathbb{S}_R^{*}(R^2) & \mathbb{S}_R^{*}(I) & 0
        \arrow[from=1-1, to=1-2] 
        \arrow[from=1-2, to=1-3, "d_2"] 
        \arrow[from=1-3, to=1-4, "d_1"] 
        \arrow[from=1-4, to=1-5] 
    \end{tikzcd}
    \]
    hence $\mathbb{S}_R^{*}(I)$ is isomorphic to the quotient of the two previous terms.
    But $\mathbb{S}_R^{*}(R^2) \cong R[x_1, x_2] \cong k[x, y, s, t]$.
    Similarly, $R \cdot \mathbb{S}_R^{*-1}(R^2) \cong \im(d_2)$ in $k[x, y, s, t]$, but
    \[
        \begin{pmatrix}
            y \\
            -x
        \end{pmatrix} \cdot f(x, y)(s, t) \mapsto (tx - sy)
    \]
    Thus,
    \[
    \mathbb{S}_R^{*}(I) \cong \frac{k[x, y, s, t]}{(tx - sy)} \cong \Rees_{k[x, y]}(x, y).
    \]
\end{solution}

% Exercise 4.22
\begin{problem}
    Let $\underline{a}$ denote a list $a_1, \ldots, a_n$ of elements of $R$, and let $F \cong R^{n}$.
    Rename $\Lambda_R^{r}(F)$ by $K_r(\underline{a})$.
    Define $R$-module homomorphisms $d_r : K_r(\underline{a}) \to K_{r-1}(\underline{a})$ on bases by setting
    \[
    d_r(e_{i_1} \wedge \cdots \wedge e_{i_r}) = \sum_{j=1}^{r}(-1)^{j-1} a_{i_j} e_{i_1} \wedge \cdots \wedge \hat{e_{i_j}} \wedge \cdots \wedge e_{i_r},
    \]
    where the hat denotes that the hatted element is omitted.
    \begin{itemize}
        \item Prove that $d_{r-1} \circ d_r = 0$.
    \end{itemize}
    Thus a collection $a_1, \ldots, a_n$ of elements of $R$ determines a complex of $R$-modules
    \[
    \begin{tikzcd}
        0 & K_n(\underline{a}) & \cdots & K_1(\underline{a}) & K_0(\underline{a}) = R & R/I & 0,
        \arrow[from=1-1, to=1-2] 
        \arrow[from=1-2, to=1-3, "d_n"]
        \arrow[from=1-3, to=1-4, "d_2"]
        \arrow[from=1-4, to=1-5, "d_1"]
        \arrow[from=1-4, to=1-5]
        \arrow[from=1-5, to=1-6] 
        \arrow[from=1-6, to=1-7] 
    \end{tikzcd}
    \]
    where $I = (a_1, \ldots, a_n)$.
    This is called the \textit{Koszul complex} of $\underline{a}$.
    \begin{itemize}
        \item Check that the complexes constructed in Exercises VI.4.13 and VI.4.14 are Koszul complexes.
    \end{itemize}
    As proven for $n = 2, 3$ in Exercises VI.4.13 and VI.4.14, the Koszul complex is exact if $(a_1, \ldots, a_n)$ is a \textit{regular} sequence in $R$, providing a free resolution fro $R/I$ in that case.
    Try to prove this in general.
\end{problem}

\begin{solution}
    We find
    \begin{align*}
        d_{r-1} \circ d_r(e_{i_1} \wedge \cdots \wedge e_{i_r}) &= d_{r-1} \left( \sum_{j=1}^{r} (-1)^{j-1} a_{i_j} e_{i_1} \wedge \cdots \wedge \hat{e_{i_j}} \wedge \cdots \wedge e_{i_r}\right)
    \end{align*}
    Since $d_{r-1}$ is a module homomorphism, this is equal to
    \begin{gather*}
        \sum_{j=1}^{r} (-1)^{j-1} a_{i_j} d_{r-1} (e_{i_1} \wedge \cdots \wedge \hat{e_{i_j}} \wedge \cdots \wedge e_{i_r})
    \end{gather*}
    or
    \[
        \sum_{j=1}^{r} (-1)^{j-1} a_{i_j} \left(
        \sum_{k=1}^{j-1} (-1)^{k-1} a_{i_k} e_{i_1} \wedge \cdots \wedge e_{i_r}
        + \sum_{\ell = j+1}^{r} (-1)^{\ell} a_{i_\ell} e_{i_1} \wedge \cdots \wedge e_{i_r} \right)
    \]
    where both inner sums omit $e_{i_k}$ and $e_{i_\ell}$ respectively.
    Then given $j < \ell$, the corresponding term in the expansion is
    \[
        (-1)^{j+\ell-1} a_{i_j} a_{i_\ell} e_{i_1} \wedge \cdots \wedge \hat{e_{i_j}} \wedge \cdots \wedge \hat{e_{i_\ell}} \wedge \cdots \wedge e_{i_r}
    \]
    Now consider the term indexed by  $j' = \ell$ and $k = j$, which yields
    \[
        (-1)^{j+\ell-2} a_{i_j} a_{i_\ell} e_{i_1} \wedge \cdots \wedge \hat{e_{i_\ell}} \wedge \cdots \wedge \hat{e_{i_\ell}} \wedge \cdots \wedge e_{i_r}
    \]
    The two are clearly additive inverses of one another, hence they cancel out.
    Since this occurs for all pairs $(j, \ell), j < \ell$ in the summation, the entire summation is 0.

    The complex in Exercise VI.4.13 is
    \[
        \begin{tikzcd}
            0 & R & R \oplus R & R & \frac{R}{(a, b)} & 0
            \arrow[from=1-1, to=1-2]
            \arrow[from=1-2, to=1-3, "d_2"]
            \arrow[from=1-3, to=1-4, "d_1"]
            \arrow[from=1-4, to=1-5, "\pi"]
            \arrow[from=1-5, to=1-6] 
        \end{tikzcd}
    \]
    where $d_1(r, s) = ra + sb$ and $d_2(t) = (bt, -at)$.
    Indeed, this is the Koszul complex of $(a, b)$ as $\Lambda_R^{1}(R^{2}) \cong R \oplus R$ and $\Lambda_R^{2}(R^2) \cong R$.
    Similarly, we can verify the differentials as
    \begin{gather*}
        d_1(e_1) = a, \quad d_1(e_2) = b \\
        d_2(e_1 \wedge e_2) = a e_2 - b e_1
    \end{gather*}
    where sending $e_1 \mapsto r$, $e_2 \mapsto s$, and $e_1 \wedge e_2 \mapsto t$ yields the desired isomorphisms.

    The complex in Exercise VI.4.14 is
    \[
    \begin{tikzcd}
        0 & R & R^{3} & R^{3} & R & \frac{R}{(a, b, c)} & 0
        \arrow[from=1-1, to=1-2]
        \arrow[from=1-2, to=1-3, "d_3"]
        \arrow[from=1-3, to=1-4, "d_2"]
        \arrow[from=1-4, to=1-5, "d_1"]
        \arrow[from=1-5, to=1-6, "\pi"]
        \arrow[from=1-6, to=1-7] 
    \end{tikzcd}
    \]
    where
    \[
    d_1 = 
    \begin{pmatrix}
        a & b & c
    \end{pmatrix}, \quad
    d_2 =
    \begin{pmatrix}
        0 & -c & -b \\
        -c & 0 & a \\
        b & a & 0
    \end{pmatrix}, \quad
    d_3 = 
    \begin{pmatrix}
        a \\
        -b \\
        c
    \end{pmatrix}.
    \]
    Again, we have isomorphisms $\Lambda_R^{1}(R^{3}) \cong R^{3}$, $\Lambda_R^{2}(R^{3}) \cong R^3$, and $\Lambda_R^{3}(R^3) \cong R$.
    Similarly, we can verify that the differentials align.
    We find
    \[
    d_1(e_1) = a, \quad d_1(e_2) = b, \quad d_1(e_3) = c 
    \]
    A basis for $\Lambda_R^2(R^3) \cong R^3$ is given by $\{ e_1 \wedge e_2, e_1 \wedge e_3, e_2 \wedge e_3 \}$.
    Computing the differential on basis vectors yields
    \[
    d_2(e_1 \wedge e_2) = a e_2 - b e_1, \quad d_2(e_1 \wedge e_3) = a e_3 - c e_1, \quad d_2(e_2 \wedge e_3) = b e_3 - c e_2
    \]
    which aligns (up to row swapping) with the matrix form of $d_2$.
    A basis for $\Lambda_R^3(R^3) \cong R$ is given by $\{e_1 \wedge e_2 \wedge e_3\}$ and computing on this basis yields
    \[
    d_3(e_1 \wedge e_2 \wedge e_3) = a e_2 \wedge e_3 - b e_1 \wedge e_3 + c e_1 \wedge e_2
    \]
    which aligns (again, up to row swapping) with the matrix form of $d_3$.

    I will return to this and try to prove that the Koszul complex of a regular sequence is exact, probably by induction.
\end{solution}
\end{document}
